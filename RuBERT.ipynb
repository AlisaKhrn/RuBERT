{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pi_Z33cfGEZh",
        "outputId": "51386494-b092-4baf-fa5e-d80dbd1af69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Разбиение по датасетам"
      ],
      "metadata": {
        "id": "hkuRKICNF4tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "import pandas as pd\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = emoji.replace_emoji(text, replace=' ')\n",
        "    text = re.sub(r'[!\"№;%:?*()\\-=\\/\\\\|@#$^&{}\\[\\]\\'.,~`\\n\\r\\t]+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def is_english(text):\n",
        "    return bool(re.fullmatch(r\"[A-Za-z0-9\\s]+\", text))\n",
        "\n",
        "def count_by_rating(df, name):\n",
        "    print(f\"\\n{name}\")\n",
        "    print(df['rating'].value_counts().sort_index())\n",
        "\n",
        "def remove_duplicates_between_datasets(df1, df2):\n",
        "    common_texts = df1['text'][df1['text'].isin(df2['text'])]\n",
        "    df1 = df1[~df1['text'].isin(common_texts)]\n",
        "    df2 = df2[~df2['text'].isin(common_texts)]\n",
        "    return df1, df2\n",
        "\n",
        "file_path_ru = \"/content/drive/MyDrive/Датасеты/RuReviews.csv\"\n",
        "labels = {\"positive\": 5, \"negative\": 1, \"neautral\": 3}\n",
        "data_ru = []\n",
        "\n",
        "with open(file_path_ru, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        words = line.split()\n",
        "        if words and words[-1] in labels:\n",
        "            rating = labels[words[-1]]\n",
        "            text = \" \".join(words[:-1])\n",
        "            data_ru.append([rating, text])\n",
        "\n",
        "ru = pd.DataFrame(data_ru, columns=[\"rating\", \"text\"])\n",
        "ru[\"text\"] = ru[\"text\"].astype(str).apply(clean_text)\n",
        "ru = ru[ru[\"text\"].str.strip() != \"\"]\n",
        "ru = ru[~ru[\"text\"].apply(is_english)]\n",
        "ru = ru[ru[\"text\"].str.split().str.len() > 1]\n",
        "ru = ru.drop_duplicates(subset='text', keep='first')\n",
        "\n",
        "wb = pd.read_csv('/content/drive/MyDrive/Датасеты/WB.csv')\n",
        "wb['text'] = wb['text'].astype(str).apply(clean_text)\n",
        "wb = wb[~wb[\"text\"].apply(is_english)]\n",
        "wb = wb[wb[\"text\"].str.split().str.len() > 3]\n",
        "wb = wb.dropna(subset=['rating', 'text'])\n",
        "wb = wb.drop_duplicates(subset='text', keep='first')\n",
        "\n",
        "ru, wb = remove_duplicates_between_datasets(ru, wb)\n",
        "\n",
        "count_by_rating(ru, \"RuReviews после 1 очистки\")\n",
        "count_by_rating(wb, \"WB после 1 очистки\")\n",
        "###########################################################################################\n",
        "test_pos = ru[ru[\"rating\"] == 5].head(7000)\n",
        "test_neu = ru[ru[\"rating\"] == 3].head(7000)\n",
        "test_neg = ru[ru[\"rating\"] == 1].head(7000)\n",
        "\n",
        "test_df1 = pd.concat([test_pos, test_neu, test_neg], ignore_index=True)\n",
        "test_df1 = test_df1.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df1 = test_df1.drop_duplicates(subset='text', keep='first')\n",
        "test_df1.to_csv(\"/content/drive/MyDrive/Датасеты/RuReviews_test_21000.csv\", index=False)\n",
        "\n",
        "ru = ru.drop(test_pos.index)\n",
        "ru = ru.drop(test_neu.index)\n",
        "ru = ru.drop(test_neg.index)\n",
        "count_by_rating(ru, \"RuReviews после 2 очистки\")\n",
        "###########################################################################################\n",
        "test_pos_ru = ru[ru[\"rating\"] == 5].head(3500)\n",
        "test_neu_ru = ru[ru[\"rating\"] == 3].head(3500)\n",
        "test_neg_ru = ru[ru[\"rating\"] == 1].head(3500)\n",
        "test_pos_wb = wb[wb[\"rating\"] == 5].head(3500)\n",
        "test_neu_wb = wb[wb[\"rating\"] == 3].head(3500)\n",
        "test_neg_wb = wb[wb[\"rating\"] == 1].head(3500)\n",
        "\n",
        "test_df2 = pd.concat([\n",
        "    test_pos_ru, test_neu_ru, test_neg_ru,\n",
        "    test_pos_wb, test_neu_wb, test_neg_wb\n",
        "], ignore_index=True)\n",
        "\n",
        "test_df2 = test_df2.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df2 = test_df2.drop_duplicates(subset='text', keep='first')\n",
        "test_df2.to_csv(\"/content/drive/MyDrive/Датасеты/RuWB_test_21000.csv\", index=False)\n",
        "wb = wb.drop(test_pos_wb.index)\n",
        "wb = wb.drop(test_neu_wb.index)\n",
        "wb = wb.drop(test_neg_wb.index)\n",
        "ru = ru.drop(test_pos_ru.index)\n",
        "ru = ru.drop(test_neu_ru.index)\n",
        "ru = ru.drop(test_neg_ru.index)\n",
        "count_by_rating(ru, \"RuReviews после 3 очистки\")\n",
        "count_by_rating(wb, \"WB после 2 очистки\")\n",
        "###########################################################################################\n",
        "val_pos_ru = ru[ru[\"rating\"] == 5].head(3500)\n",
        "val_neu_ru = ru[ru[\"rating\"] == 3].head(3500)\n",
        "val_neg_ru = ru[ru[\"rating\"] == 1].head(3500)\n",
        "val_pos_wb = wb[wb[\"rating\"] == 5].head(3500)\n",
        "val_neu_wb = wb[wb[\"rating\"] == 3].head(3500)\n",
        "val_neg_wb = wb[wb[\"rating\"] == 1].head(3500)\n",
        "\n",
        "val_df = pd.concat([\n",
        "    val_pos_ru, val_neu_ru, val_neg_ru,\n",
        "    val_pos_wb, val_neu_wb, val_neg_wb\n",
        "], ignore_index=True)\n",
        "\n",
        "val_df = val_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "val_df = val_df.drop_duplicates(subset='text', keep='first')\n",
        "val_df.to_csv(\"/content/drive/MyDrive/Датасеты/RuWB_validation_21000.csv\", index=False)\n",
        "wb = wb.drop(val_pos_wb.index)\n",
        "wb = wb.drop(val_neu_wb.index)\n",
        "wb = wb.drop(val_neg_wb.index)\n",
        "ru = ru.drop(val_pos_ru.index)\n",
        "ru = ru.drop(val_neu_ru.index)\n",
        "ru = ru.drop(val_neg_ru.index)\n",
        "count_by_rating(ru, \"RuReviews после 4 очистки\")\n",
        "count_by_rating(wb, \"WB после 3 очистки\")\n",
        "###########################################################################################\n",
        "train_pos_ru = ru[ru[\"rating\"] == 5]\n",
        "train_neu_ru = ru[ru[\"rating\"] == 3]\n",
        "train_neg_ru = ru[ru[\"rating\"] == 1]\n",
        "\n",
        "pos_deficit = 56000 - len(train_pos_ru)\n",
        "neu_deficit = 56000 - len(train_neu_ru)\n",
        "neg_deficit = 56000 - len(train_neg_ru)\n",
        "\n",
        "train_pos_wb = wb[wb[\"rating\"] == 5].head(pos_deficit)\n",
        "train_neu_wb = wb[wb[\"rating\"] == 3].head(neu_deficit)\n",
        "train_neg_wb = wb[wb[\"rating\"] == 1].head(neg_deficit)\n",
        "\n",
        "train_df4 = pd.concat([\n",
        "    train_pos_ru, train_neu_ru, train_neg_ru,\n",
        "    train_pos_wb, train_neu_wb, train_neg_wb\n",
        "], ignore_index=True)\n",
        "\n",
        "train_df4 = train_df4.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "train_df4 = train_df4.drop_duplicates(subset='text', keep='first')\n",
        "train_df4.to_csv(\"/content/drive/MyDrive/Датасеты/RuWB_train_balanced.csv\", index=False)\n",
        "wb = wb.drop(train_pos_wb.index)\n",
        "wb = wb.drop(train_neu_wb.index)\n",
        "wb = wb.drop(train_neg_wb.index)\n",
        "ru = ru.drop(train_pos_ru.index)\n",
        "ru = ru.drop(train_neu_ru.index)\n",
        "ru = ru.drop(train_neg_ru.index)\n",
        "count_by_rating(ru, \"RuReviews после 5 очистки\")\n",
        "count_by_rating(wb, \"WB после 4 очистки\")\n",
        "###########################################################################################\n",
        "train_df_35 = train_df4.copy()\n",
        "neutral_add_35 = wb[wb[\"rating\"] == 3].head(19600)\n",
        "train_df_35 = pd.concat([train_df_35, neutral_add_35], ignore_index=True)\n",
        "train_df_35 = train_df_35.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "train_df_35 = train_df_35.drop_duplicates(subset='text', keep='first')\n",
        "train_df_35.to_csv(\"/content/drive/MyDrive/Датасеты/RuWB_train_35.csv\", index=False)\n",
        "wb = wb.drop(neutral_add_35.index)\n",
        "count_by_rating(wb, \"WB после 5 очистки\")\n",
        "###########################################################################################\n",
        "train_df_55 = train_df_35.copy()\n",
        "neutral_add_55 = wb[wb[\"rating\"] == 3].head(11200)\n",
        "train_df_55 = pd.concat([train_df_55, neutral_add_55], ignore_index=True)\n",
        "train_df_55 = train_df_55.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "train_df_55 = train_df_55.drop_duplicates(subset='text', keep='first')\n",
        "train_df_55.to_csv(\"/content/drive/MyDrive/Датасеты/RuWB_train_55.csv\", index=False)\n",
        "wb = wb.drop(neutral_add_55.index)\n",
        "count_by_rating(wb, \"WB после 6 очистки\")\n",
        "###########################################################################################\n",
        "train_df_75 = train_df_55.copy()\n",
        "neutral_add_75 = wb[wb[\"rating\"] == 3].head(11200)\n",
        "train_df_75 = pd.concat([train_df_75, neutral_add_75], ignore_index=True)\n",
        "train_df_75 = train_df_75.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "train_df_75 = train_df_75.drop_duplicates(subset='text', keep='first')\n",
        "train_df_75.to_csv(\"/content/drive/MyDrive/Датасеты/RuWB_train_75.csv\", index=False)\n",
        "wb = wb.drop(neutral_add_75.index)\n",
        "count_by_rating(wb, \"WB после 7 очистки\")\n",
        "###########################################################################################\n",
        "train_df_100 = train_df_75.copy()\n",
        "neutral_add_100 = wb[wb[\"rating\"] == 3].head(14000)\n",
        "train_df_100 = pd.concat([train_df_100, neutral_add_100], ignore_index=True)\n",
        "train_df_100 = train_df_100.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "train_df_100 = train_df_100.drop_duplicates(subset='text', keep='first')\n",
        "train_df_100.to_csv(\"/content/drive/MyDrive/Датасеты/RuWB_train_100.csv\", index=False)\n",
        "wb = wb.drop(neutral_add_100.index)\n",
        "count_by_rating(wb, \"WB после 8 очистки\")\n",
        "###########################################################################################\n",
        "print('\\n Распределение отзывов по классам в получившихся датасетах \\n')\n",
        "count_by_rating(test_df1, \"RuReviews_test_21000.csv\")\n",
        "count_by_rating(test_df2, \"RuWB_test_21000.csv\")\n",
        "count_by_rating(val_df, \"RuWB_validation_21000.csv\")\n",
        "count_by_rating(train_df4, \"RuWB_train_balanced.csv\")\n",
        "count_by_rating(train_df_35, \"RuWB_train_35.csv\")\n",
        "count_by_rating(train_df_55, \"RuWB_train_55.csv\")\n",
        "count_by_rating(train_df_75, \"RuWB_train_75.csv\")\n",
        "count_by_rating(train_df_100, \"RuWB_train_100.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oKc77me9WTU",
        "outputId": "541f7f88-86d6-4134-c5df-78f2e3944672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RuReviews после 1 очистки\n",
            "rating\n",
            "1    27724\n",
            "3    27407\n",
            "5    27797\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB после 1 очистки\n",
            "rating\n",
            "1    116972\n",
            "3    120905\n",
            "5    106372\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuReviews после 2 очистки\n",
            "rating\n",
            "1    20724\n",
            "3    20407\n",
            "5    20797\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuReviews после 3 очистки\n",
            "rating\n",
            "1    17224\n",
            "3    16907\n",
            "5    17297\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB после 2 очистки\n",
            "rating\n",
            "1    113472\n",
            "3    117405\n",
            "5    102872\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuReviews после 4 очистки\n",
            "rating\n",
            "1    13724\n",
            "3    13407\n",
            "5    13797\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB после 3 очистки\n",
            "rating\n",
            "1    109972\n",
            "3    113905\n",
            "5     99372\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuReviews после 5 очистки\n",
            "Series([], Name: count, dtype: int64)\n",
            "\n",
            "WB после 4 очистки\n",
            "rating\n",
            "1    67696\n",
            "3    71312\n",
            "5    57169\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB после 5 очистки\n",
            "rating\n",
            "1    67696\n",
            "3    51712\n",
            "5    57169\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB после 6 очистки\n",
            "rating\n",
            "1    67696\n",
            "3    40512\n",
            "5    57169\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB после 7 очистки\n",
            "rating\n",
            "1    67696\n",
            "3    29312\n",
            "5    57169\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB после 8 очистки\n",
            "rating\n",
            "1    67696\n",
            "3    15312\n",
            "5    57169\n",
            "Name: count, dtype: int64\n",
            "\n",
            " Распределение отзывов по классам в получившихся датасетах \n",
            "\n",
            "\n",
            "RuReviews_test_21000.csv\n",
            "rating\n",
            "1    7000\n",
            "3    7000\n",
            "5    7000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuWB_test_21000.csv\n",
            "rating\n",
            "1    7000\n",
            "3    7000\n",
            "5    7000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuWB_validation_21000.csv\n",
            "rating\n",
            "1    7000\n",
            "3    7000\n",
            "5    7000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuWB_train_balanced.csv\n",
            "rating\n",
            "1    56000\n",
            "3    56000\n",
            "5    56000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuWB_train_35.csv\n",
            "rating\n",
            "1    56000\n",
            "3    75600\n",
            "5    56000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuWB_train_55.csv\n",
            "rating\n",
            "1    56000\n",
            "3    86800\n",
            "5    56000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuWB_train_75.csv\n",
            "rating\n",
            "1    56000\n",
            "3    98000\n",
            "5    56000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuWB_train_100.csv\n",
            "rating\n",
            "1     56000\n",
            "3    112000\n",
            "5     56000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Проверка на наличие одинаковых отзывов"
      ],
      "metadata": {
        "id": "0qxsIVSrdzjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/Датасеты/RuWB_train_balanced.csv')\n",
        "val = pd.read_csv('/content/drive/MyDrive/Датасеты/RuWB_validation_21000.csv')\n",
        "test1 = pd.read_csv('/content/drive/MyDrive/Датасеты/RuReviews_test_21000.csv')\n",
        "test2 = pd.read_csv('/content/drive/MyDrive/Датасеты/RuWB_test_21000.csv')\n",
        "\n",
        "def check(df, dataset_name):\n",
        "    duplicates = df[df.duplicated(subset=['text'], keep=False)]\n",
        "    num_duplicates = duplicates.shape[0]\n",
        "    if num_duplicates > 0:\n",
        "        print(f\"Найдено {num_duplicates} дубликатов в {dataset_name}.\")\n",
        "        print(duplicates[['text', 'rating']])\n",
        "    else:\n",
        "        print(f\"Дубликатов не найдено в {dataset_name}.\")\n",
        "    return num_duplicates\n",
        "\n",
        "num_for_train = check(train, \"train\")\n",
        "num_for_val = check(val, \"val\")\n",
        "num_for_test1 = check(test1, \"test1\")\n",
        "num_for_test2 = check(test2, \"test2\")\n",
        "\n",
        "combined_df = pd.concat([train, val, test1, test2], ignore_index=True)\n",
        "duplicates = combined_df[combined_df.duplicated(subset='text', keep=False)]\n",
        "\n",
        "if duplicates.empty:\n",
        "    print(\"Нет дубликатов в объединенном датасете\")\n",
        "else:\n",
        "    print(f\"Общее количество дубликатов: {len(duplicates)}\")\n",
        "    print(duplicates)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou25tu_Ed5q3",
        "outputId": "f638e205-4542-457b-b037-650e1ff3481b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Дубликатов не найдено в train.\n",
            "Дубликатов не найдено в val.\n",
            "Дубликатов не найдено в test1.\n",
            "Дубликатов не найдено в test2.\n",
            "Нет дубликатов в объединенном датасете\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение"
      ],
      "metadata": {
        "id": "QoYdsplAICm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class RuWBDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Датасеты/RuWB_train_55.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/Датасеты/RuWB_validation_21000.csv')\n",
        "test_ru_df = pd.read_csv('/content/drive/MyDrive/Датасеты/RuReviews_test_21000.csv')\n",
        "test_wb_df = pd.read_csv('/content/drive/MyDrive/Датасеты/RuWB_test_21000.csv')\n",
        "label_mapping = {1: 0, 3: 1, 5: 2}\n",
        "train_df['label'] = train_df['rating'].map(label_mapping)\n",
        "val_df['label'] = val_df['rating'].map(label_mapping)\n",
        "test_ru_df['label'] = test_ru_df['rating'].map(label_mapping)\n",
        "test_wb_df['label'] = test_wb_df['rating'].map(label_mapping)\n",
        "train_texts = train_df['text'].tolist()\n",
        "y_train = train_df['label'].values\n",
        "val_texts = val_df['text'].tolist()\n",
        "y_val = val_df['label'].values\n",
        "test_ru_texts = test_ru_df['text'].tolist()\n",
        "y_ru_test = test_ru_df['label'].values\n",
        "test_wb_texts = test_wb_df['text'].tolist()\n",
        "y_wb_test = test_wb_df['label'].values\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ai-forever/ruBert-base\", num_labels=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "train_dataset = RuWBDataset(train_texts, y_train, tokenizer)\n",
        "val_dataset = RuWBDataset(val_texts, y_val, tokenizer)\n",
        "test_ru_dataset = RuWBDataset(test_ru_texts, y_ru_test, tokenizer)\n",
        "test_wb_dataset = RuWBDataset(test_wb_texts, y_wb_test, tokenizer)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    return {\"macro_f1\": macro_f1}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./RuBERT_last\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.1,\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        "    metric_for_best_model=\"macro_f1\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"/content/drive/MyDrive/RuBERT_55\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/RuBERT_55\")\n",
        "\n",
        "train_predictions = trainer.predict(train_dataset)\n",
        "y_train_pred = train_predictions.predictions.argmax(axis=1)\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "print(\"F1 (Train):\", round(f1_score(y_train, y_train_pred, average='macro')*100, 2))\n",
        "\n",
        "val_predictions = trainer.predict(val_dataset)\n",
        "y_val_pred = val_predictions.predictions.argmax(axis=1)\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "print(\"F1 (Validation):\", round(f1_score(y_val, y_val_pred, average='macro')*100, 2))\n",
        "\n",
        "test_ru_predictions = trainer.predict(test_ru_dataset)\n",
        "y_test_ru_pred = test_ru_predictions.predictions.argmax(axis=1)\n",
        "print(classification_report(y_ru_test, y_test_ru_pred))\n",
        "print(\"F1 (RuReviwes test):\", round(f1_score(y_ru_test, y_test_ru_pred, average='macro')*100, 2))\n",
        "\n",
        "test_wb_predictions = trainer.predict(test_wb_dataset)\n",
        "y_test_wb_pred = test_wb_predictions.predictions.argmax(axis=1)\n",
        "print(classification_report(y_wb_test, y_test_wb_pred))\n",
        "print(\"F1 (WB test):\", round(f1_score(y_wb_test, y_test_wb_pred, average='macro')*100, 2))"
      ],
      "metadata": {
        "id": "ZV6H1H0ml67t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1cPNF0DpLEr8tjsqQvdpAHutjXlgV_NvW",
      "authorship_tag": "ABX9TyOFiY/IfyiaaWA/zFDKb+cw"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}