{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pi_Z33cfGEZh",
        "outputId": "e7a1ab4e-a3d5-4999-a750-0453ad511efd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Разбиение по датасетам"
      ],
      "metadata": {
        "id": "hkuRKICNF4tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "import pandas as pd\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = emoji.replace_emoji(text, replace=' ')\n",
        "    text = re.sub(r'[!\"№;%:?*()\\-=\\/\\\\|@#$^&{}\\[\\]\\'.,~`\\n\\r\\t]+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def is_english(text):\n",
        "    return bool(re.fullmatch(r\"[A-Za-z\\s]+\", text))\n",
        "\n",
        "def count_by_rating(df, name):\n",
        "    print(f\"\\n{name}\")\n",
        "    print(df['rating'].value_counts().sort_index())\n",
        "\n",
        "file_path_ru = \"/content/drive/MyDrive/Датасеты/RuReviews.csv\"\n",
        "labels = {\"positive\": 5, \"negative\": 1, \"neautral\": 3}\n",
        "data_ru = []\n",
        "\n",
        "with open(file_path_ru, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        words = line.split()\n",
        "        if words and words[-1] in labels:\n",
        "            rating = labels[words[-1]]\n",
        "            text = \" \".join(words[:-1])\n",
        "            data_ru.append([rating, text])\n",
        "\n",
        "ru = pd.DataFrame(data_ru, columns=[\"rating\", \"text\"])\n",
        "ru[\"text\"] = ru[\"text\"].astype(str).apply(clean_text)\n",
        "ru = ru[ru[\"text\"].str.strip() != \"\"]\n",
        "ru = ru[~ru[\"text\"].apply(is_english)]\n",
        "ru = ru[ru[\"text\"].str.split().str.len() > 1]\n",
        "counts = ru['rating'].value_counts().sort_index()\n",
        "\n",
        "wb = pd.read_csv('/content/drive/MyDrive/Датасеты/WB.csv')\n",
        "wb['text'] = wb['text'].astype(str).apply(clean_text)\n",
        "wb = wb[~wb[\"text\"].apply(is_english)]\n",
        "wb = wb[wb[\"text\"].str.split().str.len() > 2]\n",
        "wb = wb.dropna(subset=['rating', 'text'])\n",
        "counts = wb['rating'].value_counts().sort_index()\n",
        "\n",
        "count_by_rating(ru, \"RuReviews после 1 очистки\")\n",
        "count_by_rating(wb, \"WB после 1 очистки\")\n",
        "###########################################################################################\n",
        "test_pos1 = ru[ru[\"rating\"] == 5].head(8000)\n",
        "test_neu1 = ru[ru[\"rating\"] == 3].head(8000)\n",
        "test_neg1 = ru[ru[\"rating\"] == 1].head(8000)\n",
        "\n",
        "test_df1 = pd.concat([test_pos1, test_neu1, test_neg1], ignore_index=True)\n",
        "test_df1 = test_df1.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df1.to_csv(\"/content/drive/MyDrive/Датасеты/RuReviews_test_after_learning.csv\", index=False)\n",
        "ru = ru.drop(test_pos1.index)\n",
        "ru = ru.drop(test_neu1.index)\n",
        "ru = ru.drop(test_neg1.index)\n",
        "count_by_rating(ru, \"RuReviews после 2 очистки\")\n",
        "###########################################################################################\n",
        "test_pos_ru = ru[ru[\"rating\"] == 5].head(3000)\n",
        "test_neu_ru = ru[ru[\"rating\"] == 3].head(3000)\n",
        "test_neg_ru = ru[ru[\"rating\"] == 1].head(3000)\n",
        "test_pos_wb = wb[wb[\"rating\"] == 5].head(5000)\n",
        "test_neu_wb = wb[wb[\"rating\"] == 3].head(5000)\n",
        "test_neg_wb = wb[wb[\"rating\"] == 1].head(5000)\n",
        "\n",
        "test_df2 = pd.concat([\n",
        "    test_pos_ru, test_neu_ru, test_neg_ru,\n",
        "    test_pos_wb, test_neu_wb, test_neg_wb\n",
        "], ignore_index=True)\n",
        "\n",
        "test_df2 = test_df2.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df2.to_csv(\"/content/drive/MyDrive/Датасеты/RuWB_test_after_learning.csv\", index=False)\n",
        "wb = wb.drop(test_pos_wb.index)\n",
        "wb = wb.drop(test_neu_wb.index)\n",
        "wb = wb.drop(test_neg_wb.index)\n",
        "ru = ru.drop(test_pos_ru.index)\n",
        "ru = ru.drop(test_neu_ru.index)\n",
        "ru = ru.drop(test_neg_ru.index)\n",
        "count_by_rating(ru, \"RuReviews после 3 очистки\")\n",
        "count_by_rating(wb, \"WB после 2 очистки\")\n",
        "###########################################################################################\n",
        "test_pos_ru = ru[ru[\"rating\"] == 5].head(3500)\n",
        "test_neu_ru = ru[ru[\"rating\"] == 3].head(3500)\n",
        "test_neg_ru = ru[ru[\"rating\"] == 1].head(3500)\n",
        "test_df3 = pd.concat([test_pos_ru, test_neu_ru, test_neg_ru], ignore_index=True)\n",
        "test_df3 = test_df3.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df3.to_csv(\"/content/drive/MyDrive/Датасеты/RuReviews_test.csv\", index=False)\n",
        "ru = ru.drop(test_pos_ru.index)\n",
        "ru = ru.drop(test_neu_ru.index)\n",
        "ru = ru.drop(test_neg_ru.index)\n",
        "count_by_rating(ru, \"RuReviews после 4 очистки\")\n",
        "###########################################################################################\n",
        "test_pos_wb = wb[wb[\"rating\"] == 5].head(3500)\n",
        "test_neu_wb = wb[wb[\"rating\"] == 3].head(3500)\n",
        "test_neg_wb = wb[wb[\"rating\"] == 1].head(3500)\n",
        "test_df4 = pd.concat([test_pos_wb, test_neu_wb, test_neg_wb], ignore_index=True)\n",
        "test_df4 = test_df4.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df4.to_csv(\"/content/drive/MyDrive/Датасеты/WB_test.csv\", index=False)\n",
        "wb = wb.drop(test_pos_wb.index)\n",
        "wb = wb.drop(test_neu_wb.index)\n",
        "wb = wb.drop(test_neg_wb.index)\n",
        "count_by_rating(ru, \"WB после 3 очистки\")\n",
        "###########################################################################################\n",
        "val_pos_ru = ru[ru[\"rating\"] == 5].head(3500)\n",
        "val_neu_ru = ru[ru[\"rating\"] == 3].head(3500)\n",
        "val_neg_ru = ru[ru[\"rating\"] == 1].head(3500)\n",
        "val_pos_wb = wb[wb[\"rating\"] == 5].head(3500)\n",
        "val_neu_wb = wb[wb[\"rating\"] == 3].head(3500)\n",
        "val_neg_wb = wb[wb[\"rating\"] == 1].head(3500)\n",
        "\n",
        "val_df5 = pd.concat([\n",
        "    val_pos_ru, val_neu_ru, val_neg_ru,\n",
        "    val_pos_wb, val_neu_wb, val_neg_wb\n",
        "], ignore_index=True)\n",
        "\n",
        "val_df5 = val_df5.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "val_df5.to_csv(\"/content/drive/MyDrive/Датасеты/RuWB_validation.csv\", index=False)\n",
        "wb = wb.drop(val_pos_wb.index)\n",
        "wb = wb.drop(val_neu_wb.index)\n",
        "wb = wb.drop(val_neg_wb.index)\n",
        "ru = ru.drop(val_pos_ru.index)\n",
        "ru = ru.drop(val_neu_ru.index)\n",
        "ru = ru.drop(val_neg_ru.index)\n",
        "count_by_rating(ru, \"RuReviews после 5 очистки\")\n",
        "count_by_rating(wb, \"WB после 4 очистки\")\n",
        "###########################################################################################\n",
        "train_pos_ru = ru[ru[\"rating\"] == 5]\n",
        "train_neu_ru = ru[ru[\"rating\"] == 3]\n",
        "train_neg_ru = ru[ru[\"rating\"] == 1]\n",
        "\n",
        "pos_deficit = 56000 - len(train_pos_ru)\n",
        "neu_deficit = 86800 - len(train_neu_ru)\n",
        "neg_deficit = 56000 - len(train_neg_ru)\n",
        "\n",
        "train_pos_wb = wb[wb[\"rating\"] == 5].head(pos_deficit)\n",
        "train_neu_wb = wb[wb[\"rating\"] == 3].head(neu_deficit)\n",
        "train_neg_wb = wb[wb[\"rating\"] == 1].head(neg_deficit)\n",
        "\n",
        "train_df6 = pd.concat([\n",
        "    train_pos_ru, train_neu_ru, train_neg_ru,\n",
        "    train_pos_wb, train_neu_wb, train_neg_wb\n",
        "], ignore_index=True)\n",
        "\n",
        "train_df6 = train_df6.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "train_df6.to_csv(\"/content/drive/MyDrive/Датасеты/RuWB_train.csv\", index=False)\n",
        "wb = wb.drop(train_pos_wb.index)\n",
        "wb = wb.drop(train_neu_wb.index)\n",
        "wb = wb.drop(train_neg_wb.index)\n",
        "ru = ru.drop(train_pos_ru.index)\n",
        "ru = ru.drop(train_neu_ru.index)\n",
        "ru = ru.drop(train_neg_ru.index)\n",
        "count_by_rating(ru, \"RuReviews после 6 очистки\")\n",
        "count_by_rating(wb, \"WB после 5 очистки\")\n",
        "###########################################################################################\n",
        "print('\\n Распределение отзывов по классам в получившихся датасетах \\n')\n",
        "count_by_rating(test_df1, \"RuReviews_test_after_learning.csv\")\n",
        "count_by_rating(test_df2, \"RuWB_test_after_learning.csv\")\n",
        "count_by_rating(test_df3, \"RuReviews_test.csv\")\n",
        "count_by_rating(test_df4, \"WB_test.csv\")\n",
        "count_by_rating(val_df5, \"RuWB_validation.csv\")\n",
        "count_by_rating(train_df6, \"RuWB_train.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oKc77me9WTU",
        "outputId": "491e6696-61a9-4249-ea5e-add75657b00f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RuReviews после 1 очистки\n",
            "rating\n",
            "1    29426\n",
            "3    29184\n",
            "5    28874\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB после 1 очистки\n",
            "rating\n",
            "1    105000\n",
            "3    106437\n",
            "5     97334\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuReviews после 2 очистки\n",
            "rating\n",
            "1    21426\n",
            "3    21184\n",
            "5    20874\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuReviews после 3 очистки\n",
            "rating\n",
            "1    18426\n",
            "3    18184\n",
            "5    17874\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB после 2 очистки\n",
            "rating\n",
            "1    100000\n",
            "3    101437\n",
            "5     92334\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuReviews после 4 очистки\n",
            "rating\n",
            "1    14926\n",
            "3    14684\n",
            "5    14374\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB после 3 очистки\n",
            "rating\n",
            "1    14926\n",
            "3    14684\n",
            "5    14374\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuReviews после 5 очистки\n",
            "rating\n",
            "1    11426\n",
            "3    11184\n",
            "5    10874\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB после 4 очистки\n",
            "rating\n",
            "1    93000\n",
            "3    94437\n",
            "5    85334\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuReviews после 6 очистки\n",
            "Series([], Name: count, dtype: int64)\n",
            "\n",
            "WB после 5 очистки\n",
            "rating\n",
            "1    48426\n",
            "3    18821\n",
            "5    40208\n",
            "Name: count, dtype: int64\n",
            "\n",
            " Распределение отзывов по классам в получившихся датасетах \n",
            "\n",
            "\n",
            "RuReviews_test_after_learning.csv\n",
            "rating\n",
            "1    8000\n",
            "3    8000\n",
            "5    8000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuWB_test_after_learning.csv\n",
            "rating\n",
            "1    8000\n",
            "3    8000\n",
            "5    8000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuReviews_test.csv\n",
            "rating\n",
            "1    3500\n",
            "3    3500\n",
            "5    3500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "WB_test.csv\n",
            "rating\n",
            "1    3500\n",
            "3    3500\n",
            "5    3500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuWB_validation.csv\n",
            "rating\n",
            "1    7000\n",
            "3    7000\n",
            "5    7000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "RuWB_train.csv\n",
            "rating\n",
            "1    56000\n",
            "3    86800\n",
            "5    56000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение"
      ],
      "metadata": {
        "id": "QoYdsplAICm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class RuWBDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Датасеты/RuWB_train.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/Датасеты/RuWB_validation.csv')\n",
        "test_ru_df = pd.read_csv('/content/drive/MyDrive/Датасеты/RuReviews_test.csv')\n",
        "test_wb_df = pd.read_csv('/content/drive/MyDrive/Датасеты/WB_test.csv')\n",
        "label_mapping = {1: 0, 3: 1, 5: 2}\n",
        "train_df['label'] = train_df['rating'].map(label_mapping)\n",
        "val_df['label'] = val_df['rating'].map(label_mapping)\n",
        "test_ru_df['label'] = test_ru_df['rating'].map(label_mapping)\n",
        "test_wb_df['label'] = test_wb_df['rating'].map(label_mapping)\n",
        "train_texts = train_df['text'].tolist()\n",
        "y_train = train_df['label'].values\n",
        "val_texts = val_df['text'].tolist()\n",
        "y_val = val_df['label'].values\n",
        "test_ru_texts = test_ru_df['text'].tolist()\n",
        "y_ru_test = test_ru_df['label'].values\n",
        "test_wb_texts = test_wb_df['text'].tolist()\n",
        "y_wb_test = test_wb_df['label'].values\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ai-forever/ruBert-base\", num_labels=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "train_dataset = RuWBDataset(train_texts, y_train, tokenizer)\n",
        "val_dataset = RuWBDataset(val_texts, y_val, tokenizer)\n",
        "test_ru_dataset = RuWBDataset(test_ru_texts, y_ru_test, tokenizer)\n",
        "test_wb_dataset = RuWBDataset(test_wb_texts, y_wb_test, tokenizer)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    return {\"macro_f1\": macro_f1}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./RuBERT_last\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.1,\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        "    metric_for_best_model=\"macro_f1\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"/content/drive/MyDrive/RuBERT_new\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/RuBERT_new\")\n",
        "\n",
        "val_predictions = trainer.predict(val_dataset)\n",
        "y_val_pred = val_predictions.predictions.argmax(axis=1)\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "print(\"F1 (Validation):\", round(f1_score(y_val, y_val_pred, average='macro')*100, 2))\n",
        "\n",
        "test_ru_predictions = trainer.predict(test_ru_dataset)\n",
        "y_test_ru_pred = test_ru_predictions.predictions.argmax(axis=1)\n",
        "print(classification_report(y_ru_test, y_test_ru_pred))\n",
        "print(\"F1 (RuReviwes test):\", round(f1_score(y_ru_test, y_test_ru_pred, average='macro')*100, 2))\n",
        "\n",
        "test_wb_predictions = trainer.predict(test_wb_dataset)\n",
        "y_test_wb_pred = test_wb_predictions.predictions.argmax(axis=1)\n",
        "print(classification_report(y_wb_test, y_test_wb_pred))\n",
        "print(\"F1 (WB test):\", round(f1_score(y_wb_test, y_test_wb_pred, average='macro')*100, 2))\n"
      ],
      "metadata": {
        "id": "sqpGMpltIGZp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1cPNF0DpLEr8tjsqQvdpAHutjXlgV_NvW",
      "authorship_tag": "ABX9TyMEDmcAxumIALmWSuvOosba"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}